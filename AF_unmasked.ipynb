{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29def768",
   "metadata": {
    "id": "29def768"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/clami66/AF_unmasked/blob/notebook/notebooks/AF_unmasked.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6KgI44cIYON",
   "metadata": {
    "id": "d6KgI44cIYON"
   },
   "source": [
    "# AF_unmasked: a simplified notebook\n",
    "\n",
    "<img src=\"https://github.com/clami66/AF_unmasked/raw/main/fig/header.png\" height=\"200\">\n",
    "\n",
    "This notebook allows to run AF_unmasked on multimeric sequences and templates of your choice. Not all features implemented on the command line version of AF_unmasked are currently available on the notebook, but more will come later.\n",
    "\n",
    "This version of AF_unmasked relies on MMseqs2 alignments, run by the [ColabFold](https://github.com/sokrypton/ColabFold) MSA server. Some of the code on this notebook is also based or taken from the ColabFold notebook.\n",
    "\n",
    "If you use this version of AF_unmasked in your research, please cite the articles under the \"References\" section below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fadb4d-4e38-4967-9e78-fb980eef13cf",
   "metadata": {
    "cellView": "form",
    "id": "64fadb4d-4e38-4967-9e78-fb980eef13cf"
   },
   "outputs": [],
   "source": [
    "#@title Step 1: Imports\n",
    "import os\n",
    "from sys import version_info, path\n",
    "\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "import importlib_metadata\n",
    "from pathlib import Path\n",
    "from string import ascii_uppercase, ascii_lowercase\n",
    "ascii_upperlower = ascii_uppercase + ascii_lowercase\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "from alphafold.data.prepare_templates import *\n",
    "from alphafold.data.mmseqs_2_uniprot import *\n",
    "from Bio import Align, SeqIO, AlignIO\n",
    "from Bio.PDB.mmcifio import MMCIFIO\n",
    "\n",
    "from colabfold.batch import get_msa_and_templates, msa_to_str\n",
    "from colabfold.utils import DEFAULT_API_SERVER, get_commit\n",
    "from colabfold.download import download_alphafold_params\n",
    "\n",
    "from run_alphafold import predict_structure\n",
    "from alphafold.data import pipeline, pipeline_multimer\n",
    "\n",
    "from alphafold.data.tools import hmmsearch, jackhmmer\n",
    "from alphafold.data import templates\n",
    "\n",
    "from alphafold.model import model, data, config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c26503-7e21-4539-ac79-367d69e3861d",
   "metadata": {},
   "source": [
    "Clean up outputs from previous experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aea9921f-2625-4803-bc54-adeb404bf10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r AF_models/H1142/template_data\n",
    "!rm AF_models/H1142/msas/*/pdb_hits.sto\n",
    "!rm AF_models/H1142/*.pdb\n",
    "!rm AF_models/H1142/*.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853e4aa-7c0e-4c98-bd88-bc7e7a705e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_chooser = FileChooser('/home/ubuntu/software/AF_unmasked')\n",
    "fasta_chooser.title = '<b>Select .fasta file</b>'\n",
    "display(fasta_chooser)\n",
    "\n",
    "pdb_chooser = FileChooser('/home/ubuntu/software/AF_unmasked')\n",
    "pdb_chooser.title = '<b>Select .pdb file</b>'\n",
    "display(pdb_chooser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B3klWdDc79EL",
   "metadata": {
    "cellView": "form",
    "id": "B3klWdDc79EL"
   },
   "outputs": [],
   "source": [
    "fasta = fasta_chooser.value\n",
    "pdb = pdb_chooser.value\n",
    "\n",
    "jobname = os.path.basename(fasta).split(\".\")[0]\n",
    "out_dir = Path(f\"AF_models/{jobname}\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "targets = out_dir.joinpath(f\"{jobname}.fasta\")\n",
    "\n",
    "_ = shutil.copyfile(fasta, targets)\n",
    "\n",
    "if not is_fasta(targets):\n",
    "  raise ValueError(\"\"\"The input does not appear to be in fasta format\n",
    "  Example of fasta format input:\n",
    "  > H1142_A\n",
    "  GLEKDFLPLYFGWFLTK...\n",
    "  > H1142_B\n",
    "  EVQLEESGGGLVQAGGS...\n",
    "  \"\"\")\n",
    "\n",
    "with open(targets, \"r\") as f:\n",
    "  print(\"Fasta sequences:\")\n",
    "  print(f.read())\n",
    "  print()\n",
    "\n",
    "seq2chain = {}\n",
    "chain_idx = 0\n",
    "for record in SeqIO.parse(targets, \"fasta\"):\n",
    "  if record.seq not in seq2chain:\n",
    "    seq2chain[record.seq] = [ascii_upperlower[chain_idx]]\n",
    "  else:\n",
    "    seq2chain[record.seq].append(ascii_upperlower[chain_idx])\n",
    "  chain_idx += 1\n",
    "\n",
    "pdb = Path(pdb)\n",
    "\n",
    "if pdb.name.endswith(\".pdb\"):\n",
    "  template_format = \"pdb\"\n",
    "elif pdb.name.endswith(\".cif\"):\n",
    "  template_format = \"cif\"\n",
    "else:\n",
    "  raise ValueError(\"Template must be in .pdb or .cif format\")\n",
    "\n",
    "template = out_dir.joinpath(pdb.name)\n",
    "_ = shutil.copyfile(pdb, template)\n",
    "print(template)\n",
    "# template data\n",
    "template_model = load_PDB(str(template))\n",
    "template_chains = [c.id for c in template_model]\n",
    "remove_extra_chains(template_model, template_chains)\n",
    "remove_hetatms(template_model)\n",
    "template_sequences = [\n",
    "        get_fastaseq(template_model, chain) for chain in template_chains\n",
    "    ]\n",
    "\n",
    "print(\"Template sequences:\")\n",
    "for seq, chain in zip(template_sequences, template_chains):\n",
    "  print(f\"Chain {chain}: {seq}\")\n",
    "\n",
    "# target data\n",
    "target_chains, target_sequences, target_models = get_target_data(\n",
    "            [str(targets)],\n",
    "            chains=None,\n",
    "            is_fasta=True,\n",
    "        )\n",
    "assert len(target_chains) <= len(\n",
    "      template_chains\n",
    "), f\"Not enough chains in the template structure to cover all target chains. Partial templates are currently not supported on the colab version of AF_unmasked.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6e3651-8e51-480a-bdb1-c85d7789a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_per_model_widget = widgets.IntSlider(\n",
    "    value=1,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='N. predictions',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "n_recycles_widget = widgets.IntSlider(\n",
    "    value=3,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='N. recycle steps',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "dropout_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Enable dropout',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "msa_mode_widget = widgets.Dropdown(options=[\"no_MSA\", \"mmseqs2_uniref\", \"mmseqs2_uniref_env\"], value=\"no_MSA\")\n",
    "\n",
    "msa_depth_widget = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=1,\n",
    "    max=1000,\n",
    "    step=1,\n",
    "    description='MSA depth',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "inpaint_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Inpaint clashes',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "display(predictions_per_model_widget)\n",
    "display(n_recycles_widget)\n",
    "display(dropout_widget)\n",
    "display(msa_mode_widget)\n",
    "display(msa_depth_widget)\n",
    "display(inpaint_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DO1X9PpZ18UJ",
   "metadata": {
    "cellView": "form",
    "id": "DO1X9PpZ18UJ"
   },
   "outputs": [],
   "source": [
    "#@title Step 3: Set other parameters\n",
    "model_type = \"alphafold2_multimer_v2\"\n",
    "\n",
    "predictions_per_model = predictions_per_model_widget.value\n",
    "num_recycles = n_recycles_widget.value\n",
    "recycle_early_stop_tolerance = 0.5 #@param {type:\"number\"}\n",
    "use_dropout = dropout_widget.value #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown #### MSA settings\n",
    "msa_mode = msa_mode_widget.value\n",
    "\n",
    "#@markdown -  decrease `msa_depth` to rely more on template information (min: 1)\n",
    "msa_depth = msa_depth_widget.value\n",
    "\n",
    "if msa_depth == \"auto\": msa_depth = None\n",
    "\n",
    "#@markdown Template options\n",
    "inpaint_clashes = inpaint_widget.value\n",
    "align = True #@param {type:\"boolean\"}\n",
    "#superimpose = False #@param {type:\"boolean\"}\n",
    "align_tool = \"blast\"\n",
    "\n",
    "to_relax = None #@param {None, \"all\", \"best\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sU_Zn7VgPhLP",
   "metadata": {
    "cellView": "form",
    "id": "sU_Zn7VgPhLP"
   },
   "outputs": [],
   "source": [
    "#@title Step 4: provide mapping between target sequences and template chains\n",
    "\n",
    "template_preview = [f\"Chain {template_chain} (seq: {template_seq[:10]}...)\" for template_chain, template_seq in zip(template_chains, template_sequences)]\n",
    "template_c = [widgets.Dropdown(options=template_preview, value=template_preview[i]) for i, ch in enumerate(target_chains)]\n",
    "\n",
    "for i, widget in enumerate(template_c):\n",
    "  print(f\"Select template chain for fasta sequence {i+1} (seq: {target_sequences[i][:10]}...)\")\n",
    "  display(widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7678a19-f438-4752-87da-1257526ab1c9",
   "metadata": {
    "cellView": "form",
    "id": "e7678a19-f438-4752-87da-1257526ab1c9"
   },
   "outputs": [],
   "source": [
    "#@title Step 5: Perform sequence-template alignments and check the results\n",
    "#@markdown - Make sure to double check the alignments in the output to see that they are correct\n",
    "#@markdown - Run this cell up to 4 times to fill all the template slots in AlphaFold\n",
    "\n",
    "repeat_template = \"1 time\" #@param [\"1 time\", \"2 times\", \"3 times\", \"4 times\"]\n",
    "\n",
    "temp_reps = int(repeat_template.split()[0])\n",
    "template_chains = [temp.value.split()[1] for temp in template_c]\n",
    "\n",
    "if len(template_chains) != len(set(template_chains)):\n",
    "  raise ValueError(\"Must select a different template chain for each fasta sequence\")\n",
    "\n",
    "append = False\n",
    "\n",
    "mmcif_path = Path(out_dir, \"template_data\", \"mmcif_files\")\n",
    "mmcif_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for i in range(temp_reps):\n",
    "    print(f\"Filling template slot n. {i+1}...\")\n",
    "    next_id = get_next_id(mmcif_path) if append else \"0000\"\n",
    "\n",
    "    io = MMCIFIO()\n",
    "    template_mmcif_path = os.path.join(\n",
    "        out_dir, \"template_data\", \"mmcif_files\", f\"{next_id}.cif\"\n",
    "    )\n",
    "\n",
    "    if inpaint_clashes:\n",
    "        n_deleted, template_model = detect_and_remove_clashes(template_model)\n",
    "        template_sequences = [\n",
    "            get_fastaseq(template_model, chain) for chain in template_chains\n",
    "        ]\n",
    "\n",
    "    io.set_structure(template_model)\n",
    "    io.save(template_mmcif_path)\n",
    "\n",
    "    fix_mmcif(\n",
    "        template_mmcif_path, template_chains, template_sequences, \"2100-01-01\"\n",
    "    )\n",
    "\n",
    "    pdb_seqres_path = Path(out_dir, \"template_data\", \"pdb_seqres.txt\").resolve()\n",
    "    write_seqres(\n",
    "        pdb_seqres_path,\n",
    "        template_sequences,\n",
    "        template_chains,\n",
    "        seq_id=next_id,\n",
    "        append=append,\n",
    "    )\n",
    "\n",
    "    # extra flagfile for AF usage\n",
    "    af_flagfile_path = Path(out_dir, \"template_data\", \"templates.flag\")\n",
    "    if not af_flagfile_path.is_file():  # don't overwrite file if already there\n",
    "        with open(af_flagfile_path, \"w\") as flagfile:\n",
    "            flagfile.write(f\"--template_mmcif_dir={mmcif_path.resolve()}\\n\")\n",
    "            flagfile.write(f\"--pdb_seqres_database_path={pdb_seqres_path}\\n\")\n",
    "            if align:  # means we are not going to let AF overwrite pdb_hits.sto\n",
    "                flagfile.write(\"--use_precomputed_msas\\n\")\n",
    "\n",
    "    if align:\n",
    "\n",
    "        assert len(target_chains) == len(\n",
    "            template_chains\n",
    "        ), f\"The number of chains to align from target ({target_chains}) doesn't match the number of chains in the template ({template_chains}). Make sure that the files contain the same number of chains or select the chains that should be paired with --target_chains, --template_chains\"\n",
    "        for (\n",
    "            i,\n",
    "            (\n",
    "                template_chain,\n",
    "                template_sequence,\n",
    "                target_chain,\n",
    "                target_sequence,\n",
    "                target_model,\n",
    "            ),\n",
    "        ) in enumerate(\n",
    "            zip(\n",
    "                template_chains,\n",
    "                template_sequences,\n",
    "                target_chains,\n",
    "                target_sequences,\n",
    "                target_models,\n",
    "            )\n",
    "        ):\n",
    "            msa_chain = ascii_upperlower[i]\n",
    "            this_template_model = pickle.loads(pickle.dumps(template_model, -1))\n",
    "            this_target_model = pickle.loads(pickle.dumps(target_model, -1))\n",
    "            print(f\"Aligning fasta sequence {i+1} (seq: {target_sequence[0:10]}...) to template chain {template_chain} (seq: {template_sequence[0:10]}...)\")\n",
    "            alignment = do_align(\n",
    "                template_sequence,\n",
    "                this_template_model,\n",
    "                target_sequence,\n",
    "                this_target_model,\n",
    "                alignment_type=\"blast\",\n",
    "            )\n",
    "            sto_alignment = format_alignment_stockholm(\n",
    "                alignment, hit_id=next_id, hit_chain=template_chain\n",
    "            )\n",
    "\n",
    "\n",
    "            msa_path = f\"msas/{msa_chain}\"\n",
    "\n",
    "            # write alignment to file\n",
    "            Path(out_dir, msa_path).mkdir(parents=True, exist_ok=True)\n",
    "            with open(\n",
    "                Path(out_dir, msa_path, \"pdb_hits.sto\"),\n",
    "                mode=\"a\" if append else \"w\",\n",
    "            ) as pdb_hits:\n",
    "                for line in sto_alignment:\n",
    "                    pdb_hits.write(line)\n",
    "    if temp_reps > 1:\n",
    "        append = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a63c8-7e34-4f45-88aa-675355cf1818",
   "metadata": {
    "cellView": "form",
    "id": "5d2a63c8-7e34-4f45-88aa-675355cf1818"
   },
   "outputs": [],
   "source": [
    "#@title Step 6: Run the prediction with AF_unmasked\n",
    "\n",
    "if msa_mode == \"no_MSA\": # same as \"no_MSA\" on the AF_unmasked paper\n",
    "  unpaired_msa = []\n",
    "  for i, ts in enumerate(target_sequences):\n",
    "    unpaired_msa.append(f\"> seq_{i}\\n{ts}\")\n",
    "else: # Alignments rely on ColabFold's MSA server\n",
    "  print(\"Querying ColabFold's MSA server\")\n",
    "  msa_lines = None\n",
    "  use_templates = False\n",
    "  custom_template_path = None\n",
    "  pair_mode = \"unpaired\"\n",
    "  pairing_strategy = \"greedy\"\n",
    "  host_url = DEFAULT_API_SERVER\n",
    "  version = \"1.5.2\" #importlib_metadata.version(\"colabfold\")\n",
    "  commit = \"d1b8ec1\" #get_commit()\n",
    "  if commit:\n",
    "      version += f\" ({commit})\"\n",
    "  user_agent = f\"colabfold/{version}\"\n",
    "\n",
    "  unpaired_msa, paired_msa, query_seqs_unique, query_seqs_cardinality, template_features = get_msa_and_templates(jobname, target_sequences, msa_lines, out_dir, msa_mode, use_templates,\n",
    "                          custom_template_path, pair_mode, pairing_strategy, host_url, user_agent)\n",
    "\n",
    "for sequence, msa in zip(query_seqs_unique, unpaired_msa):\n",
    "  chains = seq2chain[sequence]\n",
    "  for chain in chains:\n",
    "    out_dir.joinpath(f\"msas/{chain}/bfd_uniref_hits.a3m\").write_text(msa)\n",
    "    pseudo_uniprot = open(out_dir.joinpath(f\"msas/{chain}/mmseqs2_hits.a3m\"), \"w\")\n",
    "    pseudo_uniprot.write(f\"> {chain}\\n\")\n",
    "    pseudo_uniprot.write(str(sequence))\n",
    "    pseudo_uniprot.close()\n",
    "\n",
    "data_dir = Path(\"AF_data/\")\n",
    "if not glob.glob(f\"{data_dir}/params/*_finished.txt\"):\n",
    "  print(\"downloading AF parameters...\")\n",
    "  download_alphafold_params(model_type, data_dir)\n",
    "\n",
    "template_searcher = hmmsearch.Hmmsearch(\n",
    "    binary_path=shutil.which(\"hmmsearch\"),\n",
    "    hmmbuild_binary_path=shutil.which(\"hmmbuild\"),\n",
    "    database_path=out_dir.joinpath(f\"template_data/pdb_seqres.txt\"))\n",
    "\n",
    "template_featurizer = templates.HmmsearchHitFeaturizer(\n",
    "    mmcif_dir=mmcif_path.resolve(),\n",
    "    max_template_date=\"2100-01-01\",\n",
    "    max_hits=4,\n",
    "    kalign_binary_path=shutil.which(\"kalign\"),\n",
    "    release_dates_path=None,\n",
    "    obsolete_pdbs_path=None)\n",
    "\n",
    "monomer_data_pipeline = pipeline.DataPipeline(\n",
    "    mmseqs2_binary_path=\"/home/ubuntu/miniconda3/envs/AF_unmasked/bin/mmseqs\",\n",
    "    mmseqs2_uniref_database_path=\"/home/ubuntu/software/AF_unmasked/AF_data/uniref30/uniref30_2302_db\",\n",
    "    mmseqs2_env_database_path=None,\n",
    "    jackhmmer_binary_path=\"/\",\n",
    "    hhblits_binary_path=\"/\",\n",
    "    uniref90_database_path=\".\",\n",
    "    mgnify_database_path=\"\",\n",
    "    bfd_database_path=\"\",\n",
    "    uniref30_database_path=\"\",\n",
    "    small_bfd_database_path=\"\",\n",
    "    template_searcher=template_searcher,\n",
    "    template_featurizer=template_featurizer,\n",
    "    use_small_bfd=False,\n",
    "    use_precomputed_msas=True,\n",
    "    mmseqs2_max_hits=msa_depth)\n",
    "\n",
    "data_pipeline = data_pipeline = pipeline_multimer.DataPipeline(\n",
    "          monomer_data_pipeline=monomer_data_pipeline,\n",
    "          jackhmmer_binary_path=\"/\",\n",
    "          uniprot_database_path=\"/\",\n",
    "          use_precomputed_msas=True,\n",
    "          max_uniprot_hits=10000,\n",
    "          separate_homomer_msas=True,\n",
    "          use_mmseqs2_align=True)\n",
    "\n",
    "model_names = [\"model_5_multimer_v2\"] if model_type == \"alphafold2_multimer_v2\" else [\"model_5_multimer_v3\"]\n",
    "\n",
    "model_runners = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_config = config.model_config(model_name)\n",
    "    model_config.model.num_ensemble_eval = 1\n",
    "    model_config.model.embeddings_and_evoformer.cross_chain_templates = True\n",
    "    model_config.model.num_recycle = num_recycles\n",
    "    model_config.model.global_config.eval_dropout = use_dropout\n",
    "    model_config.model.recycle_early_stop_tolerance = recycle_early_stop_tolerance\n",
    "\n",
    "    model_params = data.get_model_haiku_params(\n",
    "        model_name=model_name, data_dir=str(data_dir))\n",
    "    model_runner = model.RunModel(model_config, model_params)\n",
    "    for i in range(predictions_per_model):\n",
    "      model_runners[f'{model_name}_pred_{i}'] = model_runner\n",
    "\n",
    "predict_structure(\n",
    "        fasta_path=targets,\n",
    "        fasta_name=jobname,\n",
    "        output_dir_base=\"AF_models\",\n",
    "        data_pipeline=data_pipeline,\n",
    "        model_runners=model_runners,\n",
    "        amber_relaxer=None,\n",
    "        benchmark=False,\n",
    "        random_seed=0,\n",
    "        models_to_relax=to_relax,\n",
    "        alignments_only=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16a0e7-d6c4-44ac-9dea-4f17e4d41f90",
   "metadata": {},
   "source": [
    "Check the interface quality of a prediction against the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b351d6-358f-409e-93a5-e7efe745a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!DockQ AF_models/H1142/ranked_0.pdb examples/H1142/H1142.pdb --short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M8nHfehVTCz4",
   "metadata": {
    "cellView": "form",
    "id": "M8nHfehVTCz4"
   },
   "outputs": [],
   "source": [
    "#@title Compare predictions to the template\n",
    "import py3Dmol\n",
    "import matplotlib.pyplot as plt\n",
    "from colabfold.colabfold import plot_plddt_legend\n",
    "from colabfold.colabfold import pymol_color_list, alphabet_list\n",
    "\n",
    "rank_num = 0 #@param [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\n",
    "color = \"chain\" #@param [\"chain\", \"lDDT\", \"rainbow\"]\n",
    "show_sidechains = False #@param {type:\"boolean\"}\n",
    "show_mainchains = False #@param {type:\"boolean\"}\n",
    "\n",
    "def show_pdb(pdb_file, extension, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n",
    "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n",
    "  view.addModel(open(pdb_file,'r').read(), extension)\n",
    "\n",
    "  if color == \"lDDT\":\n",
    "    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n",
    "  elif color == \"rainbow\":\n",
    "    view.setStyle({'cartoon': {'color':'spectrum'}})\n",
    "  elif color == \"chain\":\n",
    "    chains = len(target_sequences) + 1\n",
    "    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n",
    "       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n",
    "\n",
    "  if show_sidechains:\n",
    "    BB = ['C','O','N']\n",
    "    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n",
    "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n",
    "                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
    "                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "  if show_mainchains:\n",
    "    BB = ['C','O','N','CA']\n",
    "    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n",
    "\n",
    "  view.zoomTo()\n",
    "  return view\n",
    "\n",
    "prediction_pdb = f\"{out_dir}/ranked_{rank_num}.pdb\"\n",
    "template_pdb = f\"{mmcif_path}/0000.cif\"\n",
    "\n",
    "print(\"Template\")\n",
    "show_pdb(template_pdb, \"cif\", show_sidechains, show_mainchains, color).show()\n",
    "print(\"Prediction\")\n",
    "show_pdb(prediction_pdb, \"pdb\", show_sidechains, show_mainchains, color).show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
